{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: Haotian Zhang, Zain Alam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "(describe the provided dataset that you have chosen here)\n",
    "\n",
    "Between the two given datasets, we used the Spooky Author dataset.\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "(describe your dataset here)\n",
    "\n",
    "We chose [Financial Sentiment Analysis](https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis) as our self-chosen data.\n",
    "\n",
    "Description(copied from Kaggle): The following data is intended for advancing financial sentiment analysis research. It's two datasets (FiQA, Financial PhraseBank) combined into one easy-to-use CSV file. It provides financial sentences with sentiment labels.\n",
    "\n",
    "Columns: \n",
    "Sentence: string\n",
    "Sentiment: enum\n",
    "\n",
    "We just took the sentence column in the dataset to make it a set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "uQLg8dGdt8Gr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zhanghaotian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# libs used for preprocessing\n",
    "from gensim.parsing.preprocessing import stem_text, remove_stopwords, strip_punctuation\n",
    "from nltk import ngrams\n",
    "# libs used for file reading and parsing\n",
    "from csv import reader\n",
    "from csv import writer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#write our dataset into a csv file\n",
    "def write_into_csv(data: list[list[str]], file_name: str):\n",
    "  \"\"\"This function takes in the data and writes it into a csv file.\n",
    "\n",
    "  Args:\n",
    "      data (List[List[str]]): list of sentences where each sentence is broken\n",
    "                        into list of words.\n",
    "      file_name (str): name of the file to be written\n",
    "  \"\"\"\n",
    "  with open(file_name, 'w') as f:\n",
    "    csv_writer = writer(f)\n",
    "    csv_writer.writerows(data)\n",
    "  f.close()\n",
    "\n",
    "#read our dataset from our self-generated csv file\n",
    "#placed here for running convenience\n",
    "def read_from_csv(file_name: str):\n",
    "  data = []\n",
    "  with open(file_name, 'r') as f:\n",
    "    csv_reader = reader(f)\n",
    "    for row in csv_reader:\n",
    "      data.append(row)\n",
    "  f.close()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "id": "x2IJbX_Mt8Gu"
   },
   "outputs": [],
   "source": [
    "def parse_csv(training_file_path: str, select_column:int, percentage: float) -> list[str]:\n",
    "  \"\"\"This function is used to parse input lines\n",
    "  and returns a the provided percent of data.\n",
    "\n",
    "  Args:\n",
    "      lines (List[str]): list of lines\n",
    "      percentage (int): percent of the dataset needed\n",
    "      select_column (int): column to be selected from the dataset\n",
    "  Returns:\n",
    "      List[str]: lines (percentage of dataset)\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  with open(training_file_path, \"r\", encoding=\"utf8\", errors=\"ignore\") as csvfile:\n",
    "    csv_reader = reader(csvfile)\n",
    "    #skipping header\n",
    "    header = next(csv_reader)\n",
    "\n",
    "    # line_length = len(list(csv_reader_copy))\n",
    "   \n",
    "    if header != None:\n",
    "      for row in csv_reader:\n",
    "        sentences.append(row[select_column])\n",
    "\n",
    "  random.shuffle(sentences)\n",
    "  to_return = sentences[:int(len(sentences)*percentage)]\n",
    "\n",
    "  return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(running_lines: list[str], ngrams: int) -> list[list[str]]:\n",
    "  \"\"\"This function takes in the running test and return back the\n",
    "  preprocessed text. Four tasks are done as part of this:\n",
    "    1. lower word case\n",
    "    2. remove stopwords\n",
    "    3. remove punctuation\n",
    "    4. Add - <s> and </s> for every sentence\n",
    "\n",
    "  Args:\n",
    "      running_lines (List[str]): list of lines\n",
    "\n",
    "  Returns:\n",
    "      List[List[str]]: list of sentences where each sentence is broken\n",
    "                        into list of words.\n",
    "  \"\"\"\n",
    "  preprocessed_lines = []\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  for line in running_lines:\n",
    "    lower_case_data = line.lower()\n",
    "    data_without_stop_word = remove_stopwords(lower_case_data)\n",
    "    data_without_punct = strip_punctuation(data_without_stop_word)\n",
    "    processed_data = tokenizer.tokenize(data_without_punct)\n",
    "    for i in range(1, ngrams):\n",
    "      processed_data.insert(0,\"<s>\")\n",
    "    processed_data.append(\"</s>\")\n",
    "    preprocessed_lines.append(processed_data)\n",
    "  return preprocessed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halfspooky = parse_csv(\"train.csv\", 1, 0.5)\n",
    "halfspooky = preprocessing(halfspooky, 3)\n",
    "random.shuffle(halfspooky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing into csv file as a backup\n",
    "write_into_csv(halfspooky, \"half_spooky_training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# vector_size = EMBEDDINGS_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "w2v_model_half_spooky = Word2Vec(halfspooky, vector_size=EMBEDDINGS_SIZE, window=5, min_count=1, sg=1, sorted_vocab=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size {}'.format(len(w2v_model_half_spooky.wv.key_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "w2v_model_half_spooky.wv.save_word2vec_format('half_spooky_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_set = parse_csv(\"financial_data.csv\", 0, 1)\n",
    "financial_set = preprocessing(financial_set, 3)\n",
    "random.shuffle(financial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing into csv file as a backup\n",
    "write_into_csv(financial_set, \"financial_data_training_set.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the word embeddings for the financial data\n",
    "w2v_model_financial = Word2Vec(financial_set, vector_size=EMBEDDINGS_SIZE, window=5, min_count=1, sg=1, sorted_vocab=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocabulary size for the financial data\n",
    "print('Vocab size {}'.format(len(w2v_model_financial.wv.key_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the word embeddings for the financial data\n",
    "w2v_model_financial.wv.save_word2vec_format('financial_embeddings.txt', binary=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why?\n",
    "\n",
    "Text normalization and pre-processing steps completed:\n",
    "\n",
    "Lowercasing words:\n",
    "\n",
    "Lowercasing is a common technique used to reduce vocabulary size and improve model performance. This transfers our text into plain-text. Even though some of the proper nouns would be normalized and become less significant, there're not much these words in our two datasets chosen.\n",
    "\n",
    "\n",
    "Removing punctuation:\n",
    "\n",
    "Removing puncuation is for the same reason as lowercasing to perform more clean data.\n",
    "\n",
    "\n",
    "Removing stopwords:\n",
    "\n",
    "Removing stopwords can help to further reduce vocabulary size and eliminate frequently used words that can skew model predictions. It removes the commonly-used stopwords that flattening the significance of sentences.\n",
    "\n",
    "Tokenizing the text and adding sentence separator tokens:\n",
    "\n",
    "Tokenizing the text and adding sentence separator tokens provides additional context to the words and has been shown to significantly improve accuracy in language modeling tasks such as predicting the next word. It makes the words' absolute and relative position in a sentence clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "## Write down your analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "NGRAM = 3\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "halfspooky = read_from_csv(\"half_spooky_training_set.csv\")\n",
    "half_spooky_tokenizer = Tokenizer()\n",
    "half_spooky_tokenizer.fit_on_texts(halfspooky)\n",
    "half_spooky_encoded = half_spooky_tokenizer.texts_to_sequences(halfspooky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram:int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    training_samples = []\n",
    "    for line in encoded:\n",
    "        for i in range(len(line) - ngram + 1):\n",
    "            training_samples.append(line[i:i+ngram])\n",
    "    return training_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]\n",
    "\n",
    "def generate_xy(encoded: list, ngram: int):\n",
    "    training_samples = generate_ngram_training_samples(encoded, ngram)\n",
    "    toreturn_X = []\n",
    "    toreturn_y = []\n",
    "    random.shuffle(training_samples)\n",
    "    for sample in training_samples:\n",
    "        toreturn_X.append(sample[:-1])\n",
    "        toreturn_y.append(sample[-1])\n",
    "    return toreturn_X, toreturn_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "def read_embeddings(filename, tokenizer):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    \n",
    "    # code to read the embeddings file and return the above two dicts\n",
    "    f = open(filename, 'r')\n",
    "    f.readline()\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        word_to_embedding[line[0]] = [float(x) for x in line[1:]]\n",
    "        #index to embedding refer to the index of the word in the tokenizer\n",
    "        index_to_embedding[tokenizer.word_index[line[0]]] = [float(x) for x in line[1:]]\n",
    "    return word_to_embedding, index_to_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, embedding: dict, epochs: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    a = []\n",
    "    b = []\n",
    "    for _ in range(epochs):\n",
    "        X_copy = X.copy()\n",
    "        y_copy = y.copy()\n",
    "        random.shuffle(X_copy)\n",
    "        random.shuffle(y_copy)\n",
    "        for i in range(len(X_copy)):\n",
    "            temp = []\n",
    "            for item in X_copy[i]:\n",
    "                temp.extend(embedding[item])\n",
    "            a.append(temp)\n",
    "            b.append(to_categorical(y_copy[i], num_classes= len(embedding) + 1))\n",
    "            if len(a) == num_sequences_per_batch:\n",
    "                yield (np.matrix(a), np.matrix(b))\n",
    "                a = []\n",
    "                b = []\n",
    "        yield (np.matrix(a), np.matrix(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "vgXSWdlMt8G-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 400)\n",
      "(128, 19106)\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "w, i = read_embeddings('half_spooky_embeddings.txt', half_spooky_tokenizer)\n",
    "X, y = generate_xy(half_spooky_encoded, NGRAM)\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch = len(X)//num_sequences_per_batch  # Number of batches per epoch\n",
    "train_generator = data_generator(X, y, num_sequences_per_batch, embedding=i, epochs=10)\n",
    "sample=next(train_generator) # this is how you get data out of generators\n",
    "print(sample[0].shape) # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "print(sample[1].shape) # (batch_size, |V|) to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while next(train_generator):\n",
    "  i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API\n",
    "EMBEDDINGS_SIZE = 200\n",
    "def feed_forward(embedding_size: int, vocab_size: int, ngram: int) -> Sequential:\n",
    "    '''\n",
    "    Returns a feed forward neural network model\n",
    "    '''\n",
    "    learning_rate = 0.1\n",
    "    model = Sequential()\n",
    "    model.add(Dense(ngram * EMBEDDINGS_SIZE, input_dim=(ngram - 1) * embedding_size, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "# code to train the model\n",
    "model = feed_forward(EMBEDDINGS_SIZE, len(half_spooky_tokenizer.word_index) + 1, NGRAM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1017/1017 [==============================] - 107s 105ms/step - loss: 8.9722 - accuracy: 0.0748\n",
      "Epoch 2/3\n",
      "1017/1017 [==============================] - 114s 112ms/step - loss: 8.5278 - accuracy: 0.0752\n",
      "Epoch 3/3\n",
      "1017/1017 [==============================] - 202s 199ms/step - loss: 8.4950 - accuracy: 0.0742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fcca6be37c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Start training the model\n",
    "model.fit(x=train_generator, \n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    sentence = seed\n",
    "    pres = seed\n",
    "    word_range = [i for i in range(len(tokenizer.word_index) + 1)]\n",
    "    for _ in range(n_words):\n",
    "        input_X = []\n",
    "        for item in pres:\n",
    "            input_X += w[item]\n",
    "        input_X = np.matrix(input_X)\n",
    "        output_y = model.predict(input_X)\n",
    "        #randomly choose the next word\n",
    "        output_y = list(list(output_y)[0])\n",
    "        word_chosen = random.choices(word_range, output_y)[0]\n",
    "        sentence += [tokenizer.index_word[word_chosen]]\n",
    "        if tokenizer.index_word[word_chosen] == '</s>':\n",
    "            return sentence\n",
    "        pres = sentence[-NGRAM + 1:]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_lines = []\n",
    "for i in range(50):\n",
    "    spooky_lines.append(generate_seq(model, half_spooky_tokenizer, ['<s>', '<s>'], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentence_into_files(sentences: list, filename: str):\n",
    "    f = open(filename, 'w')\n",
    "    for sentence in sentences:\n",
    "        f.write(' '.join(sentence))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "write_sentence_into_files(spooky_lines, 'spooky_lines.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences\n",
    "\n",
    "You may find it useful to run your HW 2 code on one of the datasets (or a subset of the dataset) that you used for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_txt_from_csv(filename: str):\n",
    "#     lines = read_from_csv(filename + '.csv')\n",
    "#     with open (filename + '.txt', 'w') as f:\n",
    "#         for line in lines:\n",
    "#             f.write(' '.join(line) + '\\n')\n",
    "# generate_txt_from_csv('half_spooky_trainig_set')\n",
    "# generate_txt_from_csv('financial_data_training_set')\n",
    "\n",
    "# used the code above to generate the txt files to input into hw2 code\n",
    "# 50 sentences each are stored in hw2_spooky.txt and hw2_financial.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPu_1h2t8HC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wordembeddings_starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
